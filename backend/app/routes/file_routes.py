# app/routes/file_routes.py
import json
from fastapi import APIRouter, UploadFile, File, Form, HTTPException, Depends
from sqlalchemy.orm import Session
from fastapi.responses import StreamingResponse

from app.db.database import SessionLocal
from app.services.document_service import save_document_from_analysis

# ğŸ”¥ FIX: ì˜¬ë°”ë¥¸ extractor import
from app.services.extractor import extract_text_from_file

from app.services.llm import analyze_contract
from app.models.legal import DocumentResult
from app.routes.legal import InterpretResponse
from app.nlp.extractor import build_nlp_info
from app.services.law_api import fetch_term_definitions
from app.db.models import User

import google.generativeai as genai
from app.services.llm_prompt import build_contract_analysis_prompt
from app.deps.auth import get_current_user, get_db


router = APIRouter(
    prefix="/api/files",
    tags=["files"],
)


# ---------------------------------------------------------
# DB ì¢…ì†ì„±
# ---------------------------------------------------------
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ---------------------------------------------------------
# STREAMING LLM ë¶„ì„
# ---------------------------------------------------------
@router.post("/interpret-stream")
async def interpret_stream(
    file: UploadFile = File(...),
    language: str = Form("ko"),      # ğŸ”¥ í”„ë¡ íŠ¸ì—ì„œ ë³´ë‚´ëŠ” ì–¸ì–´
    db: Session = Depends(get_db),
):
    try:
        text = await extract_text_from_file(file)
    except ValueError as e:
        return StreamingResponse(iter([f"error: {str(e)}"]), media_type="text/plain")

    nlp_info = build_nlp_info(text, language_hint=language)

    try:
        term_map = await fetch_term_definitions(nlp_info.candidate_terms)
    except Exception:
        term_map = {}

    # ğŸ”¥ ì–¸ì–´ ë°˜ì˜ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_contract_analysis_prompt(
        original_text=text,
        nlp_info=nlp_info,
        term_definitions=term_map,
        output_language=language,   # â˜… ì¶”ê°€
    )

    model = genai.GenerativeModel("gemini-2.0-flash")

    async def event_generator():

        yield json.dumps({"stage": "start", "message": "LLM ë¶„ì„ ì‹œì‘"}) + "\n"

        try:
            response = model.generate_content(
                prompt,
                stream=True,
                generation_config={
                    "temperature": 0.2,
                    "max_output_tokens": 4096,
                },
            )

            async for chunk in response:
                if hasattr(chunk, "text"):
                    yield json.dumps({
                        "stage": "chunk",
                        "content": chunk.text,
                    }) + "\n"

        except Exception as e:
            yield json.dumps({
                "stage": "error",
                "message": str(e),
            }) + "\n"
            return

        yield json.dumps({"stage": "done"}) + "\n"

    return StreamingResponse(event_generator(), media_type="text/plain")


# ---------------------------------------------------------
# í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
# ---------------------------------------------------------
@router.post("/extract-text")
async def extract_text_endpoint(file: UploadFile = File(...)):
    try:
        text = await extract_text_from_file(file)
    except ValueError as e:
        raise HTTPException(status_code=415, detail=str(e))

    return {
        "filename": file.filename,
        "preview": text[:1000],
        "length": len(text),
    }


# ---------------------------------------------------------
# íŒŒì¼ ê¸°ë°˜ ê³„ì•½ì„œ ë¶„ì„ + DB ì €ì¥
# ---------------------------------------------------------
@router.post("/interpret", response_model=InterpretResponse)
async def interpret_file(
    file: UploadFile = File(...),
    language: str = Form("ko"),            # í”„ë¡ íŠ¸ì—ì„œ ì–¸ì–´ ì „ì†¡
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    try:
        text = await extract_text_from_file(file)
    except ValueError as e:
        raise HTTPException(status_code=415, detail=str(e))

    if not text.strip():
        raise HTTPException(400, "íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    nlp_info = build_nlp_info(text, language_hint=language)

    try:
        term_map = await fetch_term_definitions(nlp_info.candidate_terms)
    except Exception:
        term_map = {}

    # ğŸ”¥ ì–¸ì–´ ë°˜ì˜ëœ LLM ë¶„ì„
    document: DocumentResult = await analyze_contract(
        original_text=text,
        nlp_info=nlp_info,
        term_definitions=term_map,
        output_language=language,   # â˜… ë°˜ë“œì‹œ í•„ìš”
    )

    summary_text = document.summary.overall_summary or "ìš”ì•½ ì—†ìŒ"

    answer_markdown = (
        "```json\n"
        + json.dumps(document.dict(), indent=2, ensure_ascii=False)
        + "\n```"
    )

    saved = save_document_from_analysis(
        db=db,
        user_id=current_user.id,
        original_text=text,
        summary=summary_text,
        answer_markdown=answer_markdown,
    )

    document.document_id = str(saved.id)

    return InterpretResponse(document=document)


# ---------------------------------------------------------
# FULL PIPELINE: OCR + NLP + LLM + DB ì €ì¥
# ---------------------------------------------------------
@router.post("/full-interpret", response_model=InterpretResponse)
async def full_interpret(
    file: UploadFile = File(...),
    language: str = Form("ko"),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user),
):
    try:
        text = await extract_text_from_file(file)
    except ValueError as e:
        raise HTTPException(status_code=415, detail=str(e))

    if not text.strip():
        raise HTTPException(400, "íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    nlp_info = build_nlp_info(text, language_hint=language)

    try:
        term_map = await fetch_term_definitions(nlp_info.candidate_terms)
    except Exception:
        term_map = {}

    # ğŸ”¥ ì–¸ì–´ ë°˜ì˜ëœ LLM ë¶„ì„
    document: DocumentResult = await analyze_contract(
        original_text=text,
        nlp_info=nlp_info,
        term_definitions=term_map,
        output_language=language,   # â˜… ì¶”ê°€
    )

    summary_text = document.summary.overall_summary or "ìš”ì•½ ì—†ìŒ"

    answer_markdown = (
        "```json\n"
        + json.dumps(document.dict(), indent=2, ensure_ascii=False)
        + "\n```"
    )

    saved = save_document_from_analysis(
        db=db,
        user_id=current_user.id,
        original_text=text,
        summary=summary_text,
        answer_markdown=answer_markdown,
    )

    document.document_id = str(saved.id)

    return InterpretResponse(document=document)
