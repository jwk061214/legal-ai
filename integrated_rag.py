import os
from dotenv import load_dotenv
from google import genai
from google.genai import types

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤
from legal_search import search_law_articles_semantically
from precedent_rag import search_precedents
from llm_service import extract_search_law_name
from deepeval_wrapper import GeminiDeepEvalLLM

# DeepEval ê´€ë ¨ ì„í¬íŠ¸ (í‰ê°€ìš©)
from deepeval.metrics import FaithfulnessMetric, AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

def get_genai_client():
    if not GEMINI_API_KEY: return None
    return genai.Client(api_key=GEMINI_API_KEY)

# --- 1. í†µí•© ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ---
def generate_integrated_answer(user_question):
    """
    1. ì§ˆë¬¸ ë¶„ì„ -> ë²•ë ¹ëª… ì¶”ì¶œ
    2. ë²•ë ¹ API ê²€ìƒ‰ -> ì‹¤ì‹œê°„ ë²¡í„° ê²€ìƒ‰ -> ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì¡°í•­ 1ê°œ ì¶”ì¶œ
    3. íŒë¡€ DB ê²€ìƒ‰ -> ê°€ì¥ ê´€ë ¨ ìˆëŠ” íŒë¡€ 1ê°œ ì¶”ì¶œ
    4. ì¢…í•© ë‹µë³€ ìƒì„±
    """
    client = get_genai_client()
    if not client: return "API í‚¤ ì˜¤ë¥˜", [], []

    logs = []
    
    # 1-1. ë²•ë ¹ëª… ì¶”ì¶œ
    logs.append("ğŸ” ì§ˆë¬¸ ë¶„ì„ ì¤‘...")
    search_params = extract_search_law_name(user_question)
    target_law = search_params.get("law_name", "ê·¼ë¡œê¸°ì¤€ë²•")
    
    # 1-2. [Source 1] ë²•ë ¹ ê²€ìƒ‰ (API + ì‹¤ì‹œê°„ ë²¡í„°)
    logs.append(f"ğŸ“œ ë²•ë ¹ ê²€ìƒ‰: '{target_law}'ì—ì„œ ê´€ë ¨ ì¡°í•­ ì°¾ëŠ” ì¤‘...")
    real_law_name, articles = search_law_articles_semantically(target_law, user_question, k=1)
    
    statute_text = articles[0] if articles else "(ê´€ë ¨ ë²• ì¡°í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
    
    # 1-3. [Source 2] íŒë¡€ ê²€ìƒ‰ (ë¯¸ë¦¬ êµ¬ì¶•ëœ ë²¡í„° DB)
    logs.append("âš–ï¸ ìœ ì‚¬ íŒë¡€ ê²€ìƒ‰ ì¤‘...")
    precedents = search_precedents(user_question, k=1)
    
    precedent_text = precedents[0].page_content if precedents else "(ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"

    # 1-4. í†µí•© ë‹µë³€ ìƒì„±
    logs.append("ğŸ¤– ë²•ë ¹ê³¼ íŒë¡€ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ ì‘ì„± ì¤‘...")
    
    prompt = f"""
    ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë²•ë¥  ìƒë‹´ AIì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ [ì°¸ê³  ë²•ë ¹]ê³¼ [ìœ ì‚¬ íŒë¡€]ë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

    [ì‚¬ìš©ì ì§ˆë¬¸]: {user_question}

    [ì°¸ê³  1: ë²•ë ¹ ({real_law_name})]:
    {statute_text}

    [ì°¸ê³  2: íŒë¡€]:
    {precedent_text}

    [ì‘ì„± ê°€ì´ë“œ]:
    1. ë¨¼ì € [ì°¸ê³  1] ë²•ë ¹ì— ê·¼ê±°í•˜ì—¬ ì›ì¹™ì ì¸ ë‹µë³€ì„ í•˜ì„¸ìš”.
    2. ê·¸ ë‹¤ìŒ [ì°¸ê³  2] íŒë¡€ë¥¼ ì¸ìš©í•˜ì—¬ ì‹¤ì œ ì ìš© ì‚¬ë¡€ë‚˜ ì˜ˆì™¸ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
    3. ë‘ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ì ì¸ ë²• ìƒì‹ì„ ë§ë¶™ì—¬ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    """

    try:
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=prompt
        )
        answer = response.text
        
        # í‰ê°€ë¥¼ ìœ„í•´ ê²€ìƒ‰ëœ ë¬¸ë§¥(Context)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        retrieved_context = [statute_text, precedent_text]
        
        return answer, retrieved_context, logs
        
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", [], logs

# --- 2. DeepEval í‰ê°€ (ë³µêµ¬ë¨) ---

def evaluate_rag_response(user_question, actual_output, retrieval_context):
    """
    ìƒì„±ëœ ë‹µë³€ì„ DeepEvalì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    - Faithfulness: ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œ(ë²•ë ¹/íŒë¡€)ì— ê·¼ê±°í•˜ëŠ”ê°€? (í™˜ê° ì²´í¬)
    - Answer Relevancy: ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆí•œê°€?
    """
    
    print("ğŸ“Š DeepEval í‰ê°€ ì‹œì‘ (Gemini ì‚¬ìš©)...")

    # 1. ì‹¬íŒê´€(Evaluator) LLM ì„¤ì •
    # deepeval_wrapper.pyì—ì„œ ì •ì˜í•œ Gemini ëª¨ë¸ ì‚¬ìš©
    gemini_evaluator = GeminiDeepEvalLLM()

    # 2. í‰ê°€ ì§€í‘œ ì„¤ì •
    faithfulness = FaithfulnessMetric(
        threshold=0.7,
        model=gemini_evaluator,
        include_reason=True
    )
    relevancy = AnswerRelevancyMetric(
        threshold=0.7,
        model=gemini_evaluator,
        include_reason=True
    )

    # 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
    test_case = LLMTestCase(
        input=user_question,
        actual_output=actual_output,
        retrieval_context=retrieval_context
    )

    # 4. í‰ê°€ ì‹¤í–‰
    faithfulness.measure(test_case)
    relevancy.measure(test_case)

    return {
        "faithfulness": {
            "score": faithfulness.score,
            "reason": faithfulness.reason,
            "pass": faithfulness.is_successful()
        },
        "relevancy": {
            "score": relevancy.score,
            "reason": relevancy.reason,
            "pass": relevancy.is_successful()
        }
    }

# ==========================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ==========================================
if __name__ == "__main__":
    print("--- í•˜ì´ë¸Œë¦¬ë“œ RAG ë° í‰ê°€ í…ŒìŠ¤íŠ¸ ---")
    q = "ì•Œë°”í•˜ë‹¤ ë‹¤ì³¤ëŠ”ë° ì‚°ì¬ ì²˜ë¦¬ ë˜ë‚˜ìš”?"
    print(f"ì§ˆë¬¸: {q}\n")
    
    # 1. ë‹µë³€ ìƒì„±
    answer, context, logs = generate_integrated_answer(q)
    
    print("\n[ì§„í–‰ ë¡œê·¸]")
    for log in logs:
        print(log)
        
    print("\n[ìµœì¢… ë‹µë³€]")
    print(answer)
    
    print("\n[í‰ê°€ ì‹œì‘]")
    # 2. í‰ê°€ ì‹¤í–‰
    try:
        eval_result = evaluate_rag_response(q, answer, context)
        print(f"Faithfulness Score: {eval_result['faithfulness']['score']}")
        print(f"Relevancy Score: {eval_result['relevancy']['score']}")
    except Exception as e:
        print(f"í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")